controller:
  cem-tf:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.02                              # sec
    mpc_horizon: 0.7                      # sec
    cem_outer_it: 5                    #how many outer iterations to use
    num_rollouts: 80          #how many rollouts per outer cem iteration
    predictor_name: "predictor_ODE_tf"    # One of ["predictor_ODE", "predictor_ODE_tf", "predictor_autoregressive_tf"]
    predictor_intermediate_steps: 10
    CEM_NET_NAME: 'GRU-6IN-32H1-32H2-5OUT-0' # Applies only if predictor name is predictor_autoregressive_tf
    cem_stdev_min: 0.1
    cem_R: 1
    cem_ccrc_weight: 1
    cem_best_k: 4
    cem_LR: 0.1
  cem-naive-grad-tf:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.02                              # sec
    mpc_horizon: 1.0                      # sec
    cem_outer_it: 5                    #how many outer iterations to use
    num_rollouts: 80          #how many rollouts per outer cem iteration
    predictor_name: "predictor_ODE_tf"    # One of ["predictor_ODE", "predictor_ODE_tf", "predictor_autoregressive_tf"]
    predictor_intermediate_steps: 10
    CEM_NET_NAME: 'GRU-6IN-32H1-32H2-5OUT-0' # Applies only if predictor name is predictor_autoregressive_tf
    cem_stdev_min: 0.05
    cem_initial_action_stdev: 0.5
    cem_R: 1
    cem_ccrc_weight: 1
    cem_best_k: 4
    cem_LR: 0.001
    gradmax_clip: 10
  cem-grad-bharadhwaj-tf:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.02                              # sec
    mpc_horizon: 1.0                      # sec
    CEM_NET_NAME: GRU-3IN-32H1-32H2-2OUT-0
    cem_LR: 0.05
    cem_R: 1
    cem_best_k: 4
    cem_outer_it: 5
    num_rollouts: 40
    cem_initial_action_stdev: 0.5
    cem_stdev_min: 0.2
    gradmax_clip: 5
    predictor_intermediate_steps: 10
    predictor_name: predictor_ODE_tf
  gradient-tf:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.02                              # sec
    mpc_horizon: 1.0                      # sec
    CEM_NET_NAME: GRU-3IN-32H1-32H2-2OUT-0
    learning_rate: 0.05
    adam_beta_1: 0.9
    adam_beta_2: 0.999
    adam_epsilon: 1.0e-07
    rtol: 1.0e-3
    gradient_steps: 5
    mpc_rollouts: 40
    initial_action_stdev: 0.5
    gradmax_clip: 5
    predictor_intermediate_steps: 10
    predictor_name: predictor_ODE_tf
  mppi-optimize-tf:
    seed: null                          # If null, random seed based on datetime is used
    mppi_LR: 0.02
    adam_beta_1: 0.4                      #default: 0.9
    adam_beta_2: 0.8                      #default: 0.999
    adam_epsilon: 1.0e-7                  #default: 1.0e-7
    gradmax_clip: 1000
    dt: 0.02                              # sec
    mpc_horizon: 1.0                      # sec
    num_rollouts: 400                     # Number of Monte Carlo samples
    predictor_name: "predictor_ODE_tf"    # One of ["predictor_ODE", "predictor_ODE_tf", "predictor_autoregressive_tf"]
    predictor_intermediate_steps: 10
    NET_NAME: 'GRU-6IN-32H1-32H2-5OUT-0'  # Applies only if predictor name is predictor_autoregressive_tf
    cc_weight: 1.0
    R: 1.0                                # How much to punish Q
    LBD: 100.0                            # Cost parameter lambda
    NU: 1000.0                            # Exploration variance
    SQRTRHOINV: 0.02
    GAMMA: 1.00                           # Future cost discount
    SAMPLING_TYPE: "interpolated"         # One of ["iid", "random_walk", "uniform", "repeated", "interpolated"]
    optim_steps: 10
  dist-adam-resamp2-tf:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.02
    mpc_horizon: 1.0
    predictor_name: "predictor_ODE_tf"    # One of ["predictor_ODE", "predictor_ODE_tf", "predictor_autoregressive_tf"]
    predictor_intermediate_steps: 10
    NET_NAME: 'GRU-6IN-32H1-32H2-5OUT-0'  # TODO: DOES NOT WORK YET!!! Applies only if predictor name is predictor_autoregressive_tf
    resamp_per: 1                         #determines after how many steps the control plans are resamples
    sample_stdev: 0.5                     #sampling variance of control action
    cem_LR: 0.002                         #learning rate parameter for adam
    adam_beta_1: 0.4    #default: 0.9      adam hyperparameter
    adam_beta_2: 0.8 #default: 0.999       adam hyperparameter
    adam_epsilon: 1.0e-7 #default: 1.0e-7    adam hyperparameter. consult https://ruder.io/optimizing-gradient-descent/ for information
    rtol: 1.0e-3
    opt_keep_k: 5                          #how many plans should be kept for warmstarting when resampling, has to be smaller or equal num_rollouts, set equal num_rollouts for degenerate case without any resampling
    num_rollouts: 20                      # number of parallel optimizations
    SAMPLING_TYPE: "interpolated" #if interpolated: linear interpolation; else iid
    warmup: True                          #whether or not we warmstart the controller. If true, first iteration will do mpc_horizon/dt*outer_its optimizations
    interpolation_step: 10                #interpolation stepsize when sampling
    outer_its: 20                         #number of optimization iterations
    gradmax_clip: 250                     #maximal gradient entry to be kept, is ||gradient||_inf <= gradmax_clip
  mppi-var-tf:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.02                              # sec
    mpc_horizon: 1.0                      # sec
    num_rollouts: 400                     # Number of Monte Carlo samples
    SAMPLING_TYPE: "interpolated" #if interpolated: linear interpolation; else iid
    interpolation_step: 10                #interpolation stepsize when sampling
    cc_weight: 1.0
    predictor_name: "predictor_ODE_tf"             # One of ["predictor_ODE", "predictor_ODE_tf", "predictor_autoregressive_tf"]
    predictor_intermediate_steps: 10
    NET_NAME: 'GRU-6IN-32H1-32H2-5OUT-0'  # TODO: DOES NOT WORK YET!!! Applies only if predictor name is predictor_autoregressive_tf
    R: 1.0                                # How much to punish Q
    # mc stands for mathematical correct, as this controller uses the formula from the paper
    LBD_mc: 10.0                          # Cost parameter lambda
    SQRTRHOINV_mc: 0.002                  # Sampling variance
    NU_mc: 20.0                           # Exploration variance
    GAMMA: 1.00                           # Future cost discount
    LR: 1000                              # Learning rate for adaption of variance, !!! Set to 0 to retrieve a mppi version in accordance with mppi paper
    STDEV_min: 0.01                       # Maximal variance for sampling
    STDEV_max: 10                         # Minimal sampling variance for sampling
    max_grad_norm: 100000                 # max norm of gradient such that ||gradient||_2
  mppi:
    seed: null                          # Seed for rng, for MPPI only, put null to set random seed (do it when you generate data for training!)
    dt: 0.02                              # sec
    mpc_horizon: 1.0                      # sec
    num_rollouts: 3500                    # Number of Monte Carlo samples
    update_every: 1                       # Cost weighted update of inputs every ... steps
    predictor_name: "predictor_ODE_tf"    # One of ["predictor_ODE", "predictor_ODE_tf", "predictor_autoregressive_tf", "predictor_autoregressive_GP"]
    predictor_intermediate_steps: 10
    NET_NAME: 'GRU-6IN-32H1-32H2-5OUT-0'  # Applies only if predictor name is predictor_autoregressive_tf
    GP_NAME: 'SGP_30'                     # Applies only if predictor name is GP
    dd_weight: 120.0
    ep_weight: 50000.0
    ekp_weight: 0.01
    ekc_weight: 5.0
    cc_weight: 1.0
    ccrc_weight: 1.0
    cost_noise: 0.0                       # Noise on stage cost weights by +/- this value, we usually set 0.5 to explore various controllers while collecting data for training, 0 othewise
    control_noise: 0.0                    # Noise on top of the calculated control input by +/- this value, we usually set 0.5 to explore various controllers while collecting data for training, 0.1 to test an not-ideal case
    R: 1.0                                # How much to punish Q
    LBD: 100.0                            # Cost parameter lambda
    NU: 1000.0                            # Exploration variance
    SQRTRHOINV: 0.02                      # Sampling variance
    GAMMA: 1.00                           # Future cost discount
    SAMPLING_TYPE: "interpolated"         # One of ["iid", "random_walk", "uniform", "repeated", "interpolated"]
    LOGGING: False                        # Collect and show detailed insights into the controller's behavior
    WASH_OUT_LEN: 100                     # Only matters if RNN used as predictor; For how long MPPI should be desactivated (replaced either with LQR or random input) to give memory units time to settle
  mppi-tf:
    seed: null                          # Seed for rng, for MPPI only, put null to set random seed (do it when you generate data for training!)
    dt: 0.02                              # sec
    mpc_horizon: 0.7                      # sec
    num_rollouts: 3500                    # Number of Monte Carlo samples
    update_every: 1                       # Cost weighted update of inputs every ... steps
    predictor_name: "predictor_ODE_tf"    # One of ["predictor_ODE", "predictor_ODE_tf", "predictor_autoregressive_tf", "predictor_autoregressive_GP"]
    predictor_intermediate_steps: 10
    NET_NAME: 'GRU-6IN-32H1-32H2-5OUT-0'  # Applies only if predictor name is predictor_autoregressive_tf
    GP_NAME: 'SGP_30'                     # Applies only if predictor name is GP
    cc_weight: 1.0
    cost_noise: 0.0                       # Noise on stage cost weights by +/- this value, we usually set 0.5 to explore various controllers while collecting data for training, 0 othewise
    control_noise: 0.0                    # Noise on top of the calculated control input by +/- this value, we usually set 0.5 to explore various controllers while collecting data for training, 0.1 to test an not-ideal case
    R: 1.0                                # How much to punish Q
    LBD: 100.0                            # Cost parameter lambda
    NU: 1000.0                            # Exploration variance
    SQRTRHOINV: 0.03                      # Sampling variance
    GAMMA: 1.00                           # Future cost discount
    SAMPLING_TYPE: "interpolated"         # One of ["iid", "random_walk", "uniform", "repeated", "interpolated"]
    LOGGING: False                        # Collect and show detailed insights into the controller's behavior
    WASH_OUT_LEN: 100                     # Only matters if RNN used as predictor; For how long MPPI should be desactivated (replaced either with LQR or random input) to give memory units time to settle
    CLIP_CONTROL_INPUT: [1.0]             # How to clip control input, symmetric
  custom-mpc-scipy:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.1
    # method: 'L-BFGS-B'
    method: 'SLSQP'
    ftol: 1.0e-8
    mpc_horizon: 10
    # weights
    wr: 0.001  # rterm
    l1: 100.0  # angle_cost
    l1_2: 0.0  # angle_sin_cost
    l2: 0.0  # angleD_cost
    l3: 0.0  # position_cost
    l4: 0.01  # positionD_cost
    m1: 0.0  # angle_sin_cost
    m2: 0.0  # angleD_cost
    m3: 0.0  # position_cost
    m4: 0.0  # positionD_cost
  do-mpc-discrete:
    dt: 0.02  # s
    mpc_horizon: 50
  do-mpc:
    seed: null                          # If null, random seed based on datetime is used
    dt: 0.02  # s
    mpc_horizon: 50
    # Perturbation factors:
    # Change of output from optimal
    p_Q: 0.00
    # Random change of cost function by factor
    p_position: 0.0
    p_positionD: 0.0
    p_angle: 0.0
    # Cost factor
    l_angle: 0.1
    l_position: 1.0
    l_positionD: 0.1
  lqr:
    seed: null  # Seed for rng, for lqr only, put null to set random seed (do it when you generate data for training!)
    Q: [10.0, 1.0, 1.0, 1.0]
    R: 10.0
    control_noise: 1.0
  pid:
    P_angle: 9.0
    I_angle: 0.0
    D_angle: 0.0
    P_position: 0.1
    I_position: 0.0
    D_position: 0.1
  mpc-opti:
    dt: 0.2  # s
    mpc_horizon: 10
  neural-imitator-tf:
    net_name: 'GRU-6IN-32H1-32H2-1OUT-0'
    PATH_TO_MODELS: './Controllers/models_for_neural_imitator_tf/'
  nn-as-mpc-tf:
    net_name: 'GRU-6IN-32H1-32H2-1OUT-0'
    PATH_TO_MODELS: './Controllers/models_for_neural_imitator_tf/'
cartpole:
  cost_function: 'quadratic-boundary-grad' # from 'quadratic-boundary', 'quadratic-boundary-grad', 'quadratic-boundary-nonconvex', 'default'
  actuator_noise: 0.0  # TODO: There already exists a noise entry
  mode: stabilization
  seed: 1873  # This is a seed for rng for CartPole instance class only. If null random seed based on datetime is used
  PATH_TO_EXPERIMENT_RECORDINGS_DEFAULT: './Experiment_Recordings/'   # Where to save experiment recording per default
  m: 0.087  # mass of pole, kg # Checked by Antonio & Tobi
  M: 0.230  # mass of cart, kg # Checked by Antonio
  L: "0.395/2.0"  # HALF (!!!) length of pend, m # Checked by Antonio & Tobi
  u_max: 2.62  # max force produced by the motor, N # Checked by Marcin
  M_fric: 4.77  # cart friction on track, N/m/s # Checked by Marcin
  J_fric: 2.5e-4  # friction coefficient on angular velocity in pole joint, Nm/rad/s # Checked by Marcin
  v_max: 0.8  # max DC motor speed, m/s, in absense of friction, used for motor back EMF model # TODO: not implemented in model, but needed for MPC
  cart_length: 4.4e-2  # m, checked by Marcin&Asude
  usable_track_length: 44.0e-2  # m, checked by Marcin&Asude
  controlDisturbance: 0.0  # disturbance, as factor of u_max # I used 0.2-0.5 for data collection
  controlBias: 0.0  # bias of control input
  g: 9.81  # absolute value of gravity acceleration, m/s^2
  k: "1.0/3.0"  # Dimensionless factor of moment of inertia of the pole with length 2L: I: (1/3)*m*(2L)^2 = (4/3)*m*(L)^2
  latency: 0.0 # s
  noise:
    noise_mode: 'OFF'
    sigma_angle: 0.0  # As measured by Asude
    sigma_position: 0.0005
    sigma_angleD: 0.075 # This is much smaller than would result from sigma_angle under assumption of iir filter+derviative calculation; the theoretical value would be 2.28
    sigma_positionD: 0.005
  num_control_inputs: 1


data_generator:
  seed: 1  # If null random seed based on datetime is used
