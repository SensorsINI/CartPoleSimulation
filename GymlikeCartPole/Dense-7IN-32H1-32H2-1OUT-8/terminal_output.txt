
Number of samples in training set: 720039
The mean number of samples from each experiment used for training is 18000.975 with variance 0.15612494995995996
Number of samples in validation set: 90004

1406/1406 [==============================] - 9s 6ms/step - loss: 1.0541

Validation loss before starting training is 1.0541337728500366
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 layers_0 (QDense)           (None, 1, 32)             256       
                                                                 
 q_activation (QActivation)  (None, 1, 32)             0         
                                                                 
 layers_1 (QDense)           (None, 1, 32)             1056      
                                                                 
 q_activation_1 (QActivation  (None, 1, 32)            0         
 )                                                               
                                                                 
 layers_2 (QDense)           (None, 1, 1)              33        
                                                                 
=================================================================
Total params: 1,345
Trainable params: 1,345
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
11250/11250 [==============================] - 154s 14ms/step - loss: 0.0836 - val_loss: 0.0500 - lr: 0.0020

Epoch 2/50
11250/11250 [==============================] - 154s 14ms/step - loss: 0.0456 - val_loss: 0.0407 - lr: 0.0020

Epoch 3/50
11250/11250 [==============================] - 154s 14ms/step - loss: 0.0395 - val_loss: 0.0373 - lr: 0.0020

Epoch 4/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0362 - val_loss: 0.0352 - lr: 0.0020

Epoch 5/50
11250/11250 [==============================] - 139s 12ms/step - loss: 0.0337 - val_loss: 0.0350 - lr: 0.0020

Epoch 6/50
11250/11250 [==============================] - 154s 14ms/step - loss: 0.0316 - val_loss: 0.0299 - lr: 0.0020

Epoch 7/50
11250/11250 [==============================] - 154s 14ms/step - loss: 0.0297 - val_loss: 0.0296 - lr: 0.0020

Epoch 8/50
11250/11250 [==============================] - 155s 14ms/step - loss: 0.0283 - val_loss: 0.0279 - lr: 0.0020

Epoch 9/50
11250/11250 [==============================] - 154s 14ms/step - loss: 0.0271 - val_loss: 0.0262 - lr: 0.0020

Epoch 10/50
11250/11250 [==============================] - 150s 13ms/step - loss: 0.0262 - val_loss: 0.0275 - lr: 0.0020

Epoch 11/50
11245/11250 [============================>.] - ETA: 0s - loss: 0.0254

Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.
11250/11250 [==============================] - 163s 14ms/step - loss: 0.0254 - val_loss: 0.0269 - lr: 0.0020

Epoch 12/50
11250/11250 [==============================] - 152s 13ms/step - loss: 0.0233 - val_loss: 0.0234 - lr: 0.0010

Epoch 13/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0229 - val_loss: 0.0243 - lr: 0.0010

Epoch 14/50
11250/11250 [==============================] - 152s 13ms/step - loss: 0.0226 - val_loss: 0.0229 - lr: 0.0010

Epoch 15/50
11250/11250 [==============================] - 153s 14ms/step - loss: 0.0223 - val_loss: 0.0238 - lr: 0.0010

Epoch 16/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0220 - val_loss: 0.0220 - lr: 0.0010

Epoch 17/50
11250/11250 [==============================] - 152s 13ms/step - loss: 0.0218 - val_loss: 0.0221 - lr: 0.0010

Epoch 18/50
11248/11250 [============================>.] - ETA: 0s - loss: 0.0216

Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0216 - val_loss: 0.0229 - lr: 0.0010

Epoch 19/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0206 - val_loss: 0.0217 - lr: 5.0000e-04

Epoch 20/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0205 - val_loss: 0.0211 - lr: 5.0000e-04

Epoch 21/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0204 - val_loss: 0.0208 - lr: 5.0000e-04

Epoch 22/50
11250/11250 [==============================] - 150s 13ms/step - loss: 0.0203 - val_loss: 0.0207 - lr: 5.0000e-04

Epoch 23/50
11250/11250 [==============================] - 152s 14ms/step - loss: 0.0203 - val_loss: 0.0207 - lr: 5.0000e-04

Epoch 24/50
11250/11250 [==============================] - 155s 14ms/step - loss: 0.0202 - val_loss: 0.0207 - lr: 5.0000e-04

Epoch 25/50
11250/11250 [==============================] - 159s 14ms/step - loss: 0.0201 - val_loss: 0.0206 - lr: 5.0000e-04

Epoch 26/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0200 - val_loss: 0.0207 - lr: 5.0000e-04

Epoch 27/50
11248/11250 [============================>.] - ETA: 0s - loss: 0.0199

Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0199 - val_loss: 0.0209 - lr: 5.0000e-04

Epoch 28/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0195 - val_loss: 0.0201 - lr: 2.5000e-04

Epoch 29/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0194 - val_loss: 0.0204 - lr: 2.5000e-04

Epoch 30/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0194 - val_loss: 0.0201 - lr: 2.5000e-04

Epoch 31/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0194 - val_loss: 0.0200 - lr: 2.5000e-04

Epoch 32/50
11250/11250 [==============================] - 152s 13ms/step - loss: 0.0193 - val_loss: 0.0198 - lr: 2.5000e-04

Epoch 33/50
11250/11250 [==============================] - 161s 14ms/step - loss: 0.0193 - val_loss: 0.0199 - lr: 2.5000e-04

Epoch 34/50
11243/11250 [============================>.] - ETA: 0s - loss: 0.0192

Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
11250/11250 [==============================] - 163s 15ms/step - loss: 0.0192 - val_loss: 0.0201 - lr: 2.5000e-04

Epoch 35/50
11250/11250 [==============================] - 152s 14ms/step - loss: 0.0190 - val_loss: 0.0198 - lr: 1.2500e-04

Epoch 36/50
11250/11250 [==============================] - 168s 15ms/step - loss: 0.0190 - val_loss: 0.0198 - lr: 1.2500e-04

Epoch 37/50
11250/11250 [==============================] - 168s 15ms/step - loss: 0.0190 - val_loss: 0.0199 - lr: 1.2500e-04

Epoch 38/50
11250/11250 [==============================] - 153s 14ms/step - loss: 0.0190 - val_loss: 0.0197 - lr: 1.2500e-04

Epoch 39/50
11250/11250 [==============================] - 150s 13ms/step - loss: 0.0189 - val_loss: 0.0198 - lr: 1.2500e-04

Epoch 40/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0189 - val_loss: 0.0197 - lr: 1.2500e-04

Epoch 41/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0189 - val_loss: 0.0195 - lr: 1.2500e-04

Epoch 42/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0189 - val_loss: 0.0196 - lr: 1.2500e-04

Epoch 43/50
11245/11250 [============================>.] - ETA: 0s - loss: 0.0189

Epoch 43: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
11250/11250 [==============================] - 152s 13ms/step - loss: 0.0189 - val_loss: 0.0196 - lr: 1.2500e-04

Epoch 44/50
11250/11250 [==============================] - 150s 13ms/step - loss: 0.0188 - val_loss: 0.0196 - lr: 6.2500e-05

Epoch 45/50
11249/11250 [============================>.] - ETA: 0s - loss: 0.0187

Epoch 45: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
11250/11250 [==============================] - 150s 13ms/step - loss: 0.0187 - val_loss: 0.0195 - lr: 6.2500e-05

Epoch 46/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0187 - val_loss: 0.0194 - lr: 3.1250e-05

Epoch 47/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0187 - val_loss: 0.0195 - lr: 3.1250e-05

Epoch 48/50
11243/11250 [============================>.] - ETA: 0s - loss: 0.0187

Epoch 48: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0187 - val_loss: 0.0195 - lr: 3.1250e-05

Epoch 49/50
11250/11250 [==============================] - 152s 14ms/step - loss: 0.0186 - val_loss: 0.0195 - lr: 1.5625e-05

Epoch 50/50
11250/11250 [==============================] - 151s 13ms/step - loss: 0.0186 - val_loss: 0.0194 - lr: 1.5625e-05

... quantizing model
Interpreting Sequential
Topology:
Layer name: input_1, layer type: InputLayer, input shapes: [[None, 1, 7]], output shape: [None, 1, 7]
Layer name: layers_0, layer type: QDense, input shapes: [[None, 1, 7]], output shape: [None, 1, 32]
Layer name: q_activation, layer type: Activation, input shapes: [[None, 1, 32]], output shape: [None, 1, 32]
Layer name: layers_1, layer type: QDense, input shapes: [[None, 1, 32]], output shape: [None, 1, 32]
Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 1, 32]], output shape: [None, 1, 32]
Layer name: layers_2, layer type: QDense, input shapes: [[None, 1, 32]], output shape: [None, 1, 1]
Model
  Precision:         ap_fixed<16,8>
  ReuseFactor:       64
  Strategy:          Resources
  BramFactor:        1000000000
  TraceOutput:       False
LayerName
  input_1
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
      weight:        ap_fixed<16,8>
      bias:          ap_fixed<16,8>
  layers_0
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
      weight:        ap_fixed<16,8>
      bias:          ap_fixed<16,8>
  layers_0_quantized_bits
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
      weight:        ap_fixed<16,8>
      bias:          ap_fixed<16,8>
  q_activation
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
  layers_1
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
      weight:        ap_fixed<16,8>
      bias:          ap_fixed<16,8>
  layers_1_quantized_bits
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
      weight:        ap_fixed<16,8>
      bias:          ap_fixed<16,8>
  q_activation_1
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
  layers_2
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
      weight:        ap_fixed<16,8>
      bias:          ap_fixed<16,8>
  layers_2_quantized_bits
    Trace:           False
    Precision
      result:        ap_fixed<16,8>
      weight:        ap_fixed<16,8>
      bias:          ap_fixed<16,8>
Interpreting Sequential
Topology:
Layer name: input_1, layer type: InputLayer, input shapes: [[None, 1, 7]], output shape: [None, 1, 7]
Layer name: layers_0, layer type: QDense, input shapes: [[None, 1, 7]], output shape: [None, 1, 32]
Layer name: q_activation, layer type: Activation, input shapes: [[None, 1, 32]], output shape: [None, 1, 32]
Layer name: layers_1, layer type: QDense, input shapes: [[None, 1, 32]], output shape: [None, 1, 32]
Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 1, 32]], output shape: [None, 1, 32]
Layer name: layers_2, layer type: QDense, input shapes: [[None, 1, 32]], output shape: [None, 1, 1]
Creating HLS model
Writing HLS project
Done

Calculating activations statistics...
For each - except for last - layer the calculation is done twice: with and without the activation function
Training Completed...                                               
 
Total time of training the network: 8107.400268564001
